{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF 주석처리된 코드 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TFIDFCaculator(object):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.tf_dict = {}\n",
    "        self.df_dict = defaultdict(int)\n",
    "\n",
    "    # 각 문서별 단어별 tf dictionary 생성\n",
    "    def build_tf_dict(self):\n",
    "        for root, dirs, files in os.walk(self.path): \n",
    "            for f in files: # 각 문서에 대해 순회\n",
    "                self.tf_dict[f] = defaultdict(int) # 문서별로 이름을 key로 갖는 dict 생성\n",
    "                with open(os.path.join(self.path, f), 'r') as fout:\n",
    "                    data = fout.read() # 각 문서의 모든 내용을 읽어들임\n",
    "                    # 각 내용을 공백으로 잘라서 소문자로 변경 후, 리스트로 만듦\n",
    "                    words = [word.lower() for word in data.split()] \n",
    "\n",
    "                    for w in words:\n",
    "                        # 앞서 만든 리스트를 순회하면서 각 단어가 출현할 때마 카운팅\n",
    "                        # f는 문서이름, w는 각 단어를 의미\n",
    "                        self.tf_dict[f][w] += 1\n",
    "                        \n",
    "                        \n",
    "    # 문서에 존재하는 모든 단어들에 대해 document frequency 생성\n",
    "    def build_df_dict(self):\n",
    "        \n",
    "        # 문서내에 존재하는 모든 단어 추출\n",
    "        all_keys = list(set([key for doc in self.tf_dict for key in self.tf_dict[doc]]))\n",
    "        \n",
    "        for key in all_keys: # 해당 단어 리스트를 순회\n",
    "            for root, dirs, files in os.walk(self.path):\n",
    "                for f in files:\n",
    "                    with open(os.path.join(self.path, f)) as fout:\n",
    "                        data = fout.read() \n",
    "                        words = [word.lower() for word in data.split()]  \n",
    "                        # 단어들이 전체 문서에서 몇번 출현했는지 빈도 확인\n",
    "                        if key in words:\n",
    "                            self.df_dict[key] += 1\n",
    "         \n",
    "    # tf, df를 이용하여 tf-idf 결과 생성\n",
    "    def get_tf_idf_dict(self, reverse = True):\n",
    "        tf_idf_dict = {}\n",
    "        \n",
    "        # doc은 각 문서의 명칭\n",
    "        for doc in self.tf_dict:\n",
    "            tf_idf_dict[doc] = {}\n",
    "            # key는 단어, value는 해당 단어가 doc에서 나타난 빈도수를 의미\n",
    "            for key, value in self.tf_dict[doc].items():\n",
    "                # 그것을 df_dict에 저장되어있는 출현한 문서수인 self.df_dict[key]로 나눔\n",
    "                tf_idf_dict[doc][key] = float(value) / self.df_dict[key]\n",
    "        \n",
    "        # 각 문서를 빈도수에 따라 정렬\n",
    "        for doc in tf_idf_dict:\n",
    "            tf_idf_dict_by_doc = tf_idf_dict[doc]\n",
    "            tf_idf_dict_by_doc = OrderedDict(sorted(tf_idf_dict_by_doc.items(), key = lambda x : x[1], reverse = reverse))\n",
    "            tf_idf_dict[doc] = tf_idf_dict_by_doc\n",
    "        return tf_idf_dict\n",
    "            \n",
    "            \n",
    "path = os.getcwd() + '/data_set'            \n",
    "tf_idf = TFIDFCaculator(path)            \n",
    "\n",
    "tf_idf.build_tf_dict()\n",
    "tf_idf.build_df_dict()\n",
    "\n",
    "for doc, tf_idf_dict in tf_idf.get_tf_idf_dict().items():\n",
    "    print doc \n",
    "    print tf_idf_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
